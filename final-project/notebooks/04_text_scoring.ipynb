{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — EDA: Text Feature Scoring\n",
    "\n",
    "**Goal:** Run all three stages of text feature engineering and validate signal quality.\n",
    "\n",
    "**Sections:**\n",
    "1. Setup\n",
    "2. Stage 4a — Keyword scoring\n",
    "3. Stage 4b — LLM rubric scoring\n",
    "4. Stage 4c — Embeddings + PCA\n",
    "5. Sanity checks\n",
    "6. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "PROJECT_ROOT = pathlib.Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "DATA_CLEAN = PROJECT_ROOT / \"data-clean\"\n",
    "OUTPUTS    = PROJECT_ROOT / \"outputs\"\n",
    "PROMPTS    = OUTPUTS / \"prompts\"\n",
    "FIGURES    = OUTPUTS / \"figures\"\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config import load_config\n",
    "from src.clean import read_parquet, write_parquet\n",
    "from src.ingest import load_transcripts_json\n",
    "from src.features_text import (\n",
    "    score_transcripts_keywords,\n",
    "    score_transcripts_llm,\n",
    "    make_embeddings,\n",
    "    build_text_features,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cfg = load_config(PROJECT_ROOT / \"configs\" / \"config.yaml\")\n",
    "print(f\"LLM model: {cfg.text.llm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomc       = read_parquet(DATA_CLEAN / \"fomc_metadata.parquet\")\n",
    "targets    = read_parquet(DATA_CLEAN / \"targets.parquet\")\n",
    "transcripts = load_transcripts_json(DATA_CLEAN / \"transcripts.json\")\n",
    "\n",
    "meeting_ids = fomc[\"meeting_id\"].tolist()\n",
    "print(f\"Meetings: {len(meeting_ids)}, transcripts loaded: {len(transcripts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage 4a — Keyword Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_scores = score_transcripts_keywords(transcripts, meeting_ids)\n",
    "kw_scores.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for ax, text_source in zip(axes, [\"statement\", \"press_conf\"]):\n",
    "    sub = kw_scores[kw_scores[\"text_source\"] == text_source].copy()\n",
    "    sub = sub.merge(fomc[[\"meeting_id\", \"announcement_et\"]], on=\"meeting_id\")\n",
    "    sub = sub.sort_values(\"announcement_et\")\n",
    "\n",
    "    colors = [\"#d62728\" if x > 0 else \"#2ca02c\" for x in sub[\"net_hawkish\"]]\n",
    "    ax.bar(range(len(sub)), sub[\"net_hawkish\"], color=colors)\n",
    "    ax.axhline(0, color=\"k\", lw=0.8)\n",
    "    ax.set_title(f\"Keyword net_hawkish — {text_source}\")\n",
    "    ax.set_xlabel(\"Meeting index\")\n",
    "    ax.set_ylabel(\"Hawkish − Dovish count\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Keyword-Based Hawkish/Dovish Scores\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 4b — LLM Rubric Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already scored (avoid re-running API calls)\n",
    "llm_cache = DATA_CLEAN / \"llm_scores_cache.parquet\"\n",
    "\n",
    "if llm_cache.exists():\n",
    "    print(f\"Loading cached LLM scores from {llm_cache}\")\n",
    "    llm_scores = read_parquet(llm_cache)\n",
    "else:\n",
    "    print(f\"Running LLM scoring (82 API calls via {cfg.text.llm_model}) ...\")\n",
    "    llm_scores = score_transcripts_llm(\n",
    "        transcripts=transcripts,\n",
    "        meeting_ids=meeting_ids,\n",
    "        prompt_path=PROMPTS / f\"rubric_{cfg.text.prompt_version}.txt\",\n",
    "        llm_model=cfg.text.llm_model,\n",
    "        log_dir=OUTPUTS,\n",
    "    )\n",
    "    write_parquet(llm_scores, llm_cache)\n",
    "    print(f\"Cached to {llm_cache}\")\n",
    "\n",
    "llm_scores.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_cols = [\"hawkish_dovish\", \"inflation_focus\", \"labor_focus\",\n",
    "               \"recession_risk\", \"uncertainty_score\", \"forward_guidance_strength\",\n",
    "               \"balance_sheet_mention\"]\n",
    "\n",
    "print(\"LLM score distributions:\")\n",
    "print(llm_scores.groupby(\"text_source\")[rubric_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 7))\n",
    "rubric_display = rubric_cols[:7]  # 7 dimensions\n",
    "\n",
    "for text_source, row_axes in zip([\"statement\", \"press_conf\"], [axes[0], axes[1]]):\n",
    "    sub = llm_scores[llm_scores[\"text_source\"] == text_source]\n",
    "    for ax, col in zip(row_axes, rubric_display):\n",
    "        ax.hist(sub[col].dropna(), bins=10, edgecolor=\"white\", color=\"steelblue\")\n",
    "        ax.set_title(f\"{col}\\n({text_source})\", fontsize=8)\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    # hide last subplot if 7 dims and 8 slots\n",
    "    if len(row_axes) > len(rubric_display):\n",
    "        row_axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"LLM Rubric Score Distributions\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline: hawkish_dovish score vs actual rate changes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n",
    "\n",
    "for ax, text_source in zip(axes, [\"statement\", \"press_conf\"]):\n",
    "    sub = llm_scores[llm_scores[\"text_source\"] == text_source].copy()\n",
    "    sub = sub.merge(fomc[[\"meeting_id\", \"announcement_et\", \"rate_change\"]], on=\"meeting_id\")\n",
    "    sub = sub.sort_values(\"announcement_et\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(sub[\"announcement_et\"], sub[\"hawkish_dovish\"], \"b-o\", ms=4, label=\"hawkish_dovish (LLM)\")\n",
    "    ax2.bar(sub[\"announcement_et\"], sub[\"rate_change\"] * 100, width=20, alpha=0.4, color=\"orange\", label=\"Rate change (bps)\")\n",
    "    ax.set_ylabel(\"hawkish_dovish score\", color=\"b\")\n",
    "    ax2.set_ylabel(\"Rate change (bps)\", color=\"orange\")\n",
    "    ax.set_title(f\"{text_source}: LLM score vs rate change\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 4c — Embeddings + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already computed\n",
    "emb_cache = DATA_CLEAN / \"embeddings_cache.parquet\"\n",
    "\n",
    "if emb_cache.exists():\n",
    "    print(f\"Loading cached embeddings from {emb_cache}\")\n",
    "    emb_scores = read_parquet(emb_cache)\n",
    "else:\n",
    "    print(\"Computing embeddings (requires sentence-transformers) ...\")\n",
    "    emb_scores = make_embeddings(\n",
    "        transcripts=transcripts,\n",
    "        meeting_ids=meeting_ids,\n",
    "        n_components_min=cfg.text.embedding_dim_min,\n",
    "        n_components_max=cfg.text.embedding_dim_max,\n",
    "        figures_dir=FIGURES,\n",
    "    )\n",
    "    write_parquet(emb_scores, emb_cache)\n",
    "    print(f\"Cached to {emb_cache}\")\n",
    "\n",
    "emb_cols = [c for c in emb_scores.columns if c.startswith(\"emb_pc_\")]\n",
    "print(f\"Embedding components: {len(emb_cols)} per text_source\")\n",
    "emb_scores.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show PCA variance plots if saved\n",
    "for text_source in [\"statement\", \"press_conf\"]:\n",
    "    fig_path = FIGURES / f\"pca_variance_{text_source}.png\"\n",
    "    if fig_path.exists():\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(str(fig_path)))\n",
    "    else:\n",
    "        print(f\"PCA plot not found at {fig_path} (run embedding stage above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key sanity check: hawkish_dovish (LLM) should correlate with net_hawkish (keyword)\n",
    "stmt_kw  = kw_scores[kw_scores[\"text_source\"] == \"statement\"][[\"meeting_id\", \"net_hawkish\", \"net_hawkish_norm\"]]\n",
    "stmt_llm = llm_scores[llm_scores[\"text_source\"] == \"statement\"][[\"meeting_id\", \"hawkish_dovish\"]]\n",
    "sanity = stmt_kw.merge(stmt_llm, on=\"meeting_id\")\n",
    "\n",
    "corr_kw_llm = sanity[[\"net_hawkish\", \"hawkish_dovish\"]].corr().iloc[0, 1]\n",
    "print(f\"Keyword net_hawkish vs LLM hawkish_dovish correlation: r = {corr_kw_llm:.3f}\")\n",
    "\n",
    "if corr_kw_llm > 0.3:\n",
    "    print(\"✓ Positive correlation — LLM and keyword scores broadly agree\")\n",
    "elif corr_kw_llm > 0:\n",
    "    print(\"⚠ Weak positive correlation — check a few low-agreement meetings\")\n",
    "else:\n",
    "    print(\"✗ Negative or zero correlation — review LLM prompt or word lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(sanity[\"net_hawkish\"], sanity[\"hawkish_dovish\"], alpha=0.7, s=50)\n",
    "z = np.polyfit(sanity[\"net_hawkish\"].dropna(), sanity[\"hawkish_dovish\"].dropna(), 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(sanity[\"net_hawkish\"].min(), sanity[\"net_hawkish\"].max(), 50)\n",
    "ax.plot(x_line, p(x_line), \"r--\", lw=1.5)\n",
    "ax.set_title(f\"Keyword vs LLM Hawkish Score (r={corr_kw_llm:.2f})\")\n",
    "ax.set_xlabel(\"Keyword net_hawkish\")\n",
    "ax.set_ylabel(\"LLM hawkish_dovish\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: hawkish_dovish should negatively correlate with 2Y yield change\n",
    "# (hawkish = market expects tighter policy = 2Y yield rises AFTER announcement)\n",
    "# Use statement score vs statement-window 2Y yield implied return\n",
    "# (We don't have 2Y target directly, but we can check vs fomc rate_change)\n",
    "sanity2 = stmt_llm.merge(fomc[[\"meeting_id\", \"rate_change\"]], on=\"meeting_id\").dropna()\n",
    "corr_hd_rc = sanity2[[\"hawkish_dovish\", \"rate_change\"]].corr().iloc[0, 1]\n",
    "print(f\"hawkish_dovish vs rate_change correlation: r = {corr_hd_rc:.3f}\")\n",
    "print(\"Expected: positive (hawkish meetings tend to be hike meetings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score variance check — do scores differentiate meetings?\n",
    "print(\"LLM score variance (should be >0 for all dims):\")\n",
    "stmt_llm_scores = llm_scores[llm_scores[\"text_source\"] == \"statement\"]\n",
    "print(stmt_llm_scores[rubric_cols].std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all text features\n",
    "feat_text = kw_scores.merge(llm_scores.drop(columns=[\"llm_model_id\"], errors=\"ignore\"), on=[\"meeting_id\", \"text_source\"], how=\"outer\")\n",
    "feat_text = feat_text.merge(emb_scores, on=[\"meeting_id\", \"text_source\"], how=\"outer\")\n",
    "\n",
    "write_parquet(feat_text, DATA_CLEAN / \"features_text.parquet\")\n",
    "print(f\"Saved {len(feat_text)} rows, {feat_text.shape[1]} columns\")\n",
    "print(f\"Columns: {feat_text.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Stage | Features | Notes |\n",
    "|---|---|---|\n",
    "| 4a Keyword | net_hawkish, net_hawkish_norm, balance_sheet_kw, uncertainty_kw | Deterministic |\n",
    "| 4b LLM Rubric | 7 rubric dimensions | Claude API, cached |\n",
    "| 4c Embeddings | emb_pc_1 ... emb_pc_N | PCA from sentence-transformers |\n",
    "\n",
    "**Next:** `05_model_results.ipynb` — run the full model ladder and compare RMSE across rungs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
